{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS161 Lecture 2\n",
    "\n",
    "Source: [CS161 Lecture 2](http://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst and best case analysis\n",
    "\n",
    "Last time we gave the formal definitions of O, omega, and theta. Today I will elaborate a little bit more on how these relate to algorithms, and also how it relates to whether something is the worst case complexity or the best case complexity.\n",
    "\n",
    "It is easy to think big-O complexity means the same thing as \"worst case time complexity\" and big-omega complexity means the same thing as \"best case time complexity.\" Actually, those two things are completely separate.\n",
    "\n",
    "__Worst case vs. best case:__ Your algorithm has different runtimes on different inputs, and a lot of times we want to know how fast the algorithm runs on an input of a certain size. Worst case runtime means that you are feeding the worst possible input (of that size) into your algorithm. Best case runtime means that you are feeding the best possible input into your algorithm. For an input of size n, perhaps the worst case runtime is T(N) = 2n^2 + 5, and the best case runtime is 3n.\n",
    "\n",
    "__Big-O vs. Big-Omega:__ This refers to __a way of bounding complicated functions by a simpler function, so it is easier to work with them__. For example, the worst case runtime T(n) = 2n^2 + 5 can be bounded from below by a constant times n^2, so we say that T(N) is omega(n^2). It can also be bounded from above by a constant times n^2 (for sufficiently large values of n), so we say that T(n) is O(n^2) as well.\n",
    "\n",
    "Similarly, the best case runtime T(n) = 3n can be bounded both below and above by a constant times n, so we say that the best case runtime is O(n), omega(n) and theta(n).\n",
    "\n",
    "You can even bound things that are not runtimes of algorithms. For example, the function f(x) = x^5 + 4x^4 + 3x^3 + 2x log x is theta(x^5_, even though f is not the runtime of an algorithm, and x does not correspond to an input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and conquer algorithms\n",
    "\n",
    "Correctness proofs follow design patterns - if you see one correctness proof, it will help you write similar correctness proofs. Algorithms also follow design patterns. You have you \"__incremental algorithms__,\" where you __build the solution one step at a time__. (Insertion sort was a good example of this, where we started with only the first element being sorted, and then incrementally added to the sorted part of the array on every iteration.) We also have greedy algorithms, and dynamic programming algorithms (we'll explain what those are in a few weeks). And today we'll be covering a broad class of algorithms called divide-and-conquer algorithms.\n",
    "\n",
    "Divide and conquer algorithms have two steps:\n",
    "1. Break the problem up into a series of smaller problems that are exactly like the original problem\n",
    "2. Combine the answers to the subproblems to produce an answer to the larger problem.\n",
    "\n",
    "Before we go into full-blown divide and conquer mode, first I want to show you a recursive algorithm for raising a number to an integer power. Then I will show you merge sort, which is a classic example of a divide and conquer algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentiation via repeated squaring\n",
    "\n",
    "Suppose we want to find 3^n for some nonnegative integer n. The naive way to do it is uisng a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Exponentiator(n):\n",
    "    answer = 1\n",
    "    for i in range(n):\n",
    "        answer = answer * 3\n",
    "    return answer\n",
    "\n",
    "Exponentiator(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs in O(n) time, since the body of the for loop takes O(1) time to execute, and the for loop test is executed n+1 times. (Like the algorithm in the previous lecture, this is n+1 instead of n because the for loop test also gets executed on the iteration where we break out of the loop.)\n",
    "\n",
    "However, we can get a faster algorithm if we write this recursively. By that I mean we can write a function that calls itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Exponentiator(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n % 2 == 0:     # n is even\n",
    "        x = Exponentiator(n/2)\n",
    "        return x*x\n",
    "    else:                # n is odd\n",
    "        x = Exponentiator((n-1)/2)\n",
    "        return 3*x*x\n",
    "    \n",
    "Exponentiator(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is important to separate this calculation into even and odd cases so that the input to Exponentiator is always an integer. If this algorithm were extended to fractional inputs, it might never terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "We can prove the correctness of this using __strong induction__.\n",
    "\n",
    "Base case: Suppose n = 0. Then Exponentiator returns 1, which is correct, since 3^0 = 1.\n",
    "\n",
    "Inductive step: Suppose Exponentiator(k) produces correct output for all integers k < n (this is the inductive hypothesis). Then we must prove that Exponentiator(n) produces correct output, i.e. that it returns 3^n.\n",
    "\n",
    "Suppose n is even. Then the algorithm return x^2, where x = Exponentiator(n/2). By the inductive hypothesis, x = 3^(n/2). So the algorithm returns (3^(n/2))^2 = 3^n.\n",
    "\n",
    "Suppose n is odd. Then x = Exponentiator((n-1)/2) = 3^((n-1)/2). The algorithm returns 3x^2 = 3(3(n-1)/2)^2 = 3(3^(n-1)) = 3^n.\n",
    "\n",
    "This shows that Exponentiator produces correct output for all integers n >= 0. (Note that this induction proof wouldn't work if n could be any real number.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime\n",
    "\n",
    "Let T(n) be the runtime of Exponentiator on input n. Then we can express the runtime as a recurrence relation:\n",
    "\n",
    "        c1,                for n = 0\n",
    "T(n) =  T(n/2) + c2        for n even\n",
    "        T((n-1)/2) + c3    for n odd\n",
    "        \n",
    "Observe that if the pseducode had said \"return Exponentiator(n/2) * Exponentiator(n/2)\" instead of \"x = Exponentiator(n/2); return x * x\", the runtime would be substantially worse, since we would have to make two calls to Exponentiator every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge sort is a divide and conquer algorithm for sorting arrays. To sort an array, first you split it into two arrays of roughly equal size. Then sort each of those arrays using merge sort, and merge the two sorted arrays.\n",
    "\n",
    "__Example:__ Suppose you are sorint A = [5,2,4,7,1,3,2,6]. We divide this into the arrays [5,2,4,7] and [1,3,2,6], and MergeSort each of those arrays. To sort [5,2,4,7], we divide it into the arrays [5,2] and [4,7], and MergeSort each of those arrays. To sort [5,2], we divide it into the arrays [5] and [2]. At this point we have reached the base case, so MergeSorting the array [5] just returns the array [5], and MergeSorting the array [2] just returns the array [2]. But now we need to merge together [5] and [2], which gives us [2,5]. Merging together [2,5] and [4,7] gives us [2,4,5,7]. Finally, merging together [2,4,5,7] and [1,2,3,6] gives us [1,2,2,3,4,5,6,7].\n",
    "\n",
    "![Figure 2.4](images/merge-sort.png)\n",
    "\n",
    "__Figure 2.4__ The operation of merge sort on the array A = [5,2,4,7,1,3,2,6]. The lengths of the sorted sequences being merged increase as the algorithm progresses from bottom to top.\n",
    "\n",
    "Here is the pseudocide for Merge Sort. It's not the same that's in the book (actually it's from last quarter's lecture notes). The book version is on page 31 and 34, and the main difference is that the book version is more formal, and modifies the existing array instead of creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MergeSort(A):\n",
    "    n = length(A)\n",
    "    if n <= 1:\n",
    "        return A\n",
    "    L = MergeSort(A[1 .. floor(n/2)])\n",
    "    R = MergeSort(A[floor(n/2)+1 .. n])\n",
    "    return Merge(L, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's how to do a __merge__. Assume you are given two sorted arrays, L and R, and you want to produce a sorted array that has all the elements in both L and R.\n",
    "\n",
    "Then you keep a pointer to the first element of each array. You know that the first element of the final array will either be the first element of L or the first element of R. So you choose the lowest one, and copy it to the result array. Then you increment the corresponding pointer so it points to the second element in that array instead of the first.\n",
    "\n",
    "Now say we picked from array L in the first step. Then the second-lowest element in the result array either has to be the second element of L, or the first element of R. So once again, we choose the lowest one, and copy it to the result array, and increment the corresponding pointer. We keep doing this until we run out of elements in both the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge(L, R):\n",
    "    m = length(L) + length(R)\n",
    "    S = empty array of size m\n",
    "    i = 1; j = 1\n",
    "    for k = 1 to m:\n",
    "        if L[i] <= R[j]:\n",
    "            S[k] = L[i]\n",
    "            i += 1\n",
    "        else:\n",
    "            S[k] = R[j]\n",
    "            j += 1\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The astute observer may ask what happens if one of the arrays gets depleted first. This code, as written, would result in a runtime exception, as we try to access and element that is outside the bounds of the array. In order to fixx this, we add the \"sentinel element\" INF to the end of both arrays. INF will always be larger than any element that is actually in the array, so this will make us always pick from the other array after the first array has been depleted.\n",
    "\n",
    "__Example__: Suppose we are merging L = [2,4,5,7] with R = [1,2,3,6,8,9,11]. We start with i = 1, and j = 1. Observing that R[1] < L[1], we let S[1] = 1 be the first element of the result array, and set j = 2. Now L[1] <= R[2], so we set S[2] = 2, and i = 2. Now R[2] < L[2], so S[3] = 2, and j = 3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [5, 2, 4, 7, 1, 3, 2, 6]\n",
      "After: [1, 2, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "def MergeSort(A):            # takes in a list of numbers\n",
    "    n = len(A)\n",
    "    if n <= 1:               # if only 1 element in list, just return list\n",
    "        return A\n",
    "    L = MergeSort(A[:n//2])  # subcall to MergeSort with left half\n",
    "    R = MergeSort(A[n//2:])  # subcall to MergeSort with right half\n",
    "    return Merge(L, R)       # return Merge of L and R\n",
    "\n",
    "def Merge(L, R):             # takes in sorted L and sorted R lists\n",
    "    m = len(L) + len(R)      # total len of output S\n",
    "    S = [None] * m           # initialize output list S\n",
    "    i = 0; j = 0             # ptrs to smallest element of L and R\n",
    "    L.append(float('inf'))   # append sentinel element to L\n",
    "    R.append(float('inf'))   # append sentinel element to R\n",
    "    for k in range(m):       # get output of each element in S by merging\n",
    "        if L[i] <= R[j]:\n",
    "            S[k] = L[i]\n",
    "            i += 1\n",
    "        else:\n",
    "            S[k] = R[j]\n",
    "            j += 1\n",
    "    return S\n",
    "\n",
    "A = [5,2,4,7,1,3,2,6]\n",
    "print(f'Before: {A}')\n",
    "print(f'After: {MergeSort(A)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime analysis\n",
    "\n",
    "First we will analyze the runtime of the Merge subroutine. The first and third lines take constant time. Assume the second line, initializing the array S, takes time proportional to m. The body of the for loop takes a constant amount of time to execute, and the body gets executed _m_ times. So the entire subroutine just takes c1 + c2*m + c3 + c4*m, which we will just write as O(m).\n",
    "\n",
    "Now we will analyze the runtime of MergeSort. To simplify our analysis, we will assume that the length of the array is a power of 2. Under those simplifying assumtions, the MergeSort runtime for an array of length _n_ can be written as T(n) = 2T(n/2) + theta(n) (for n > 1), and T(n) = theta(1) (for n = 1). The 2T(n/2) term comes from the fact that we are making two sub-calls to MergeSort, each on arrays of length n/2. The theta(n) is the time it takes to merge the arrays. There are also some constant terms arising from line 1 and line 2, but those constants get overwhelmed by the theta(n) term, so we can drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of correctness\n",
    "\n",
    "#### Proof of _Merge_\n",
    "\n",
    "First we will prove the correctness of the Merge subroutine. The loop invariant we will use is\n",
    "\n",
    "(Loop invariant is a property of a program loop that is true before and after each iteration. It is a logical assertion, sometimes checked within the code by an assertion call. Knowing its invariant(s) is essential in understanding the effect of a loop.)\n",
    "\n",
    "__Loop invariant:__ At the start of each iteration k of the for loop, the nonempty part of S contains the k - 1 smallest elements of L and R, in sorted order. Moreover, L[i] and R[j] are the smallest elements of their arrays that have not been copied to S.\n",
    "\n",
    "To use this loop invariant, we must prove the three loop invariant conditions.\n",
    "\n",
    "__Initialization:__ The loop invariant holds prior to the first iteration of the loop. Here, i = j = 1, and S is completely empty. L[1] is the smallest element of L, while R[1] is the smallest element of R, so the initialization step holds.\n",
    "\n",
    "__Maintenance:__ To see that each iteration maintains the loop invariant, suppose without loss of generality that L[i] <= R[j]. Then L[i] is the smallest element not yet copied to S. The current nonempty part of S consists of the k - 1 smallest elements, so after the loop is over and L[i] is copied to S, the nonempty part of S will consist of the k smallest elements. Incrementing k (in the for loop update) and i restablishes the loop invariant for the next iteration.\n",
    "\n",
    "__Termination:__ At termination, k = m + 1. By the loop invariant, S contains the m smallest elements of L and R, in sorted order. This is the result that we wanted (i.e. the merging of the two sorted arrays to produce a new sorted array).\n",
    "\n",
    "#### Proof of _MergeSort_\n",
    "\n",
    "Here we use strong induction to prove that MergeSort correctly sorts all arrays A of size n.\n",
    "\n",
    "__Base case:__ Suppose n = 1, which means the array A has one element. Then MergeSort just returns A, which is sorted.\n",
    "\n",
    "__Inductive step:__ Suppose MergeSort correctly sorts all arrays of size less than n. Then the first half and second half of the array are both arrays of size less than n. So line 4 correctly sorts the first half of the array to produce L, and line 5 correctly sorts the second half of the array to produce R. Since L and R are now sorted, and we have already proven that the Merge subroutine is correct, MergeSort returns the sorted array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
